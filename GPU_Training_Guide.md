# GPU Training on Jubail HPC Guide

To access NYUAD's HPC Jubail, follow  the instructions [here](https://crc-docs.abudhabi.nyu.edu/hpc/accounts/index.html) to request an HPC account. Once your account is approved, make sure to complete the [HPC Training](https://crc-docs.abudhabi.nyu.edu/hpc/training/index.html#training) quiz. You will be able to submit jobs on the HPC two business days after passing the quiz.

On a Linux/Mac, login to Jubail with your NetID using the following command, while connected to NYU Network:
```bash
ssh <NetID>@jubail.abudhabi.nyu.edu
```
If connecting from outside NYU Network, or using windows, refer to [this](https://crc-docs.abudhabi.nyu.edu/hpc/system/access_jubail.html)

## Contents
- [File Systems](#file-systems)
- [Using Conda](#using-conda)
  - [One-time set up](#one-time-set-up-of-Miniconda-on-HPC)
  - [Creating a conda environment](#creating-a-conda-environment)
- [Using venv and pip](#using-venv-and-pip)
- [Submitting jobs on the HPC](#submitting-jobs-on-the-hpc)
- [Installing libraries](#installing-libraries)
  - [Pytorch](#pytorch)
  - [TensorFlow](#tensorflow)

## File Systems
The HPC contains 4 file storage systems:
- [`$HOME`](https://crc-docs.abudhabi.nyu.edu/hpc/storage/home_scratch.html); contains source code and executables.
- [`$SCRATCH`](https://crc-docs.abudhabi.nyu.edu/hpc/storage/home_scratch.html); used to run jobs from.
- [`$WORK`](https://crc-docs.abudhabi.nyu.edu/hpc/storage/work.html); mountable on your local workstation, best suited to quick post-processing, analysis and visualization, without moving your data.
- [`$ARCHIVE`](https://crc-docs.abudhabi.nyu.edu/hpc/storage/archive.html); used for long-term storage.

Use `myquota` to check your quota status on each storage system.


## Using conda

  - ### One time set up of Miniconda on HPC
  ```bash
  module load miniconda
  source ~/.bashrc
  ```
  Miniconda will allow you to create and use conda environments

  - ### Creating a conda environment
  ```bash
  conda create -n <env-name>
  ```

  The environment is created in `$HOME` by default. However, in `$HOME`, the quota is limited. Hence, it is recommended that you create the environment in `$SCRATCH` as follows:
  ```bash
  conda create -p /scratch/<NetID>/<path-to-env>/<env-name>
  ```

  #### View the list of conda environments
    ```bash
    conda env list
    ```
  #### View the list of packages in the active conda environment
    ```bash
    conda list
  ```

  #### Activating the conda environment
  ```bash
  conda activate <path-to-env>
  ```
  #### Install packages
  A simple installation looks like this
  ```bash
  conda install <package-name>
  ```
  #### Deactivate the environment
   ```bash
   conda deactivate
   ```

  #### Clean up cache files
  Your quota can get filled with cache files. You can use `conda clean -a` to clean up the cache files generated by conda.

  #### Use a specific Python version
  ```bash
  conda create -p <path-to-env> python=<python-version>   # EXAMPLE:  python=3.8
  ```

  #### Delete an environment
  ```bash
  conda env remove -p <path-to-env>   # or  -n <env-name>
  ```

More tips can be found in the [HPC Documentation](https://crc-docs.abudhabi.nyu.edu/hpc/training/miniconda.html?highlight=pip), and in this [conda cheat sheet](https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf).



## Using venv and pip
  - #### To create a virtual environment
  ```bash
  python -m venv <venv-name>
  ```
  The python version will be inhereted from the active conda environment

  - #### Activate the virtual environment
  ```bash
  source <name-or-path-to-venv>/bin/activate
  ```
  You can deactivate the `venv` using the command `deactivate`

  - #### View the list of packages installed
  ```bash
  pip list
  ```

  - #### Clean up cache files
  ```bash
  pip cache purge
  ```

### Use a specific python version
  - #### By using the available Python modules on the HPC
  1. Run `module avail python` to view the available python versions. Example:

  ```bash
  $ module avail python
   --------------------- /share/apps/NYUAD5/modules/SOFTWARE ---------------------
   python/2.7.18  python/3.8.6  python/3.9.0
  ```

  2. Load the preferred version. For example:
  ```bash
  module load python/3.8.6
  ```

  3. Create a virtual environment
  ```bash
  python -m venv <name-or-path-to-venv>
  ```
  4. Activate the virtual environment
  ```bash
  source <name-or-path-to-venv>/bin/activate
  ```
  3. Check the python version you are running
  ```bash
  python --version
  ```
  - #### By using a Conda environment
  1. [Create a Conda environment](#creating-a-conda-environment) with the preferred Python version. For example:
  ```bash
  conda create -p <path-to-env> python=3.7
  ```

  2. Activate the conda environment
  ```bash
  conda activate <path-to-env>
  ```

  3. Create a virtual environment inside the activated Conda environment
  ```bash
  python -m venv <name-or-path-to-venv>
  ```

  4. Deactivate the Conda environment
  ```bash
  conda deactivate
  ```
  > should not delete this conda environment that the venv is sourcing python from ..?

  5. Activate your virtual environment
  ```bash
  python <name-or-path-to-venv>/bin/activate
  ```
  Check the python version with `$ python --version`.

  `$ which python` will show the path of where your python is sourced from.





## Submitting jobs on the HPC:
- Put your data in `$SCRATCH` and run your jobs from there
- Create and activate an environment with all your dependencies
- Do not allocate more than the maximum resources allowed for a job

  A script of a job using GPU would look like this:
  ```bash
  #!/bin/bash                                           # This is always the first line

  # SBATCH options go here as follows

  #SBATCH --time=<time>                                 # EXAMPLE   -t 30:00:00   asks for 30 hours runtime
  #SBATCH --mem=<memory>                                # Default units are megabytes
  #SBATCH --partition=<partition-names>                 # Specify   -p nvidia  to request a GPU card
  #SBATCH --gres=gpu:<number-of-gpus>                   # EXAMPLE   --gres=gpu:1  to use 1 GPU node
  #SBATCH --ntasks=<number-of-tasks-per-node>           # A default of 1 task per core. A multiple of 128 tasks per node.
  #SBATCH --mail-type=<BEGIN,END,FAIL,etc>              # Sends an email notification at the specified events
  #SBATCH --mail-user=<NetID>@nyu.edu
  #SBATCH --output=<name-and-path-of-output-file>       # EXAMPLE using Slurm job id:   -o /scratch/user1234/project1/slurm-%j.out

  # If using conda environment
  source ~/.bashrc                                      # Activate conda when submitting a Slurm job
  conda activate <path-to-env>                          # Activate the conda environment

  # If using venv
  source <path-to-venv>/bin/activate                    # Activate the virtual environment

  # Script to Execute
  python3 myproject.py
  ```

    #### Submit a job with `sbatch`
    ```bash
    sbatch <jobscript>
    ```

    #### Show active jobs
    ```bash
    squeue # On the login node
    ```
    For more verbose information on a job
    ```bash
    scontrol show job <jobid>
    ```

    #### Cancel a job
    ```bash
    scancel <jobid>
    ```

    ### Interactive Sessions
    #### Start an interactive session with `srun`
    Interactive sessions allow you to use compute nodes interactively.
    ```bash
    srun --pty /bin/bash
    ```
    The following is an interactive session with some options specified
    ```bash
    srun --mem=60000 --gres=gpu:2 -p nvidia --pty /bin/bash
    ```

    To exit an interactive sesssion use `exit`

More on [Job Management](https://crc-docs.abudhabi.nyu.edu/hpc/jobs/index.html)




## Installing libraries
### Pytorch

#### Clone an existing environment on the HPC
You can use the available Pytorch installation on the HPC by [cloning the Pytorch environments](https://crc-docs.abudhabi.nyu.edu/hpc/software/hpc_pytorch.html?highlight=pytorch).
  1. First, list the available conda environments
  ```bash
  conda env list
  ```
  which will show the list of conda environments
  ```
  base                  *  /share/apps/NYUAD5/miniconda/3-4.11.0
  gnuplot                  /share/apps/NYUAD5/miniconda/3-4.11.0/envs/gnuplot
  jupyter_env              /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyter_env
  jupyterlab               /share/apps/NYUAD5/miniconda/3-4.11.0/envs/jupyterlab
  openmm                   /share/apps/NYUAD5/miniconda/3-4.11.0/envs/openmm
  pytorch-1.11.0           /share/apps/NYUAD5/miniconda/3-4.11.0/envs/pytorch-1.11.0
  raspa                    /share/apps/NYUAD5/miniconda/3-4.11.0/envs/raspa
  tensorboard              /share/apps/NYUAD5/miniconda/3-4.11.0/envs/tensorboard
  tensorflow-2.4.1         /share/apps/NYUAD5/miniconda/3-4.11.0/envs/tensorflow-2.4.1
  ```  
  And copy the name or path of the environment you wish to clone

  2. If using Miniconda
  ```bash
  conda create -p <path-to-env> --clone pytorch-1.11.0
  ```


#### Install Pytorch with `conda` or `pip`
Steps to install Pytorch with GPU suppport
- Start by launching an [interactive session](#interactive-sessions)
- Run `nvidia-smi` and check the CUDA version. For example
```
    $ nvidia-smi

    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 460.106.00   Driver Version: 460.106.00   CUDA Version: 11.2     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                               |                      |               MIG M. |
    |===============================+======================+======================|
    |   0  Tesla V100-PCIE...  Off  | 00000000:86:00.0 Off |                    0 |
    | N/A   30C    P0    26W / 250W |      0MiB / 32510MiB |      0%      Default |
    |                               |                      |                  N/A |
    +-------------------------------+----------------------+----------------------+

    +-----------------------------------------------------------------------------+
    | Processes:                                                                  |
    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
    |        ID   ID                                                   Usage      |
    |=============================================================================|
    |  No running processes found                                                 |
    +-----------------------------------------------------------------------------+
```
- Exit the interactive session
- Go to [pytoch.org](https://pytorch.org) for the latest version, or select a [previous version of pytorch](https://pytorch.org/get-started/previous-versions/)
- Choose the your preferred configurations with a CUDA version less than or equal to the resource available, and copy the command
  ```bash
  # EXAMPLE installing pytorch 1.10 with support for CUDA 11.2 using pip in a venv
  pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html
  ```
- Activate your conda environment or venv
- Run the command

#### Check GPU usage in Python using Pytorch
```python
import torch
```
Check if CUDA is available. This statement should return `True` if CUDA is available.
```python
torch.cuda.is_available()
```
To get the total memory:
```python
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
torch.cuda.get_device_properties(device).total_memory
```

More `torch.cuda` functions on [this](https://pytorch.org/docs/stable/cuda.html#) page.

### TensorFlow
#### Clone existing TensorFlow environment on the HPC
Use the same steps above to [clone an existing environment](#clone-an-existing-environment-on-the-hpc).

#### Install TensorFlow with `pip`
1. [Create and activate a Conda environment](#creating-a-conda-environment)
2. Install CUDA and cuDNN
```bash
conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0
```
3. Use `pip` to install TensorFlow
```bash
pip install tensorflow
```

```
create conda env
install cudatoolkit
optional: create venv
install tensorflow with pip
```


Or choose your preferred version from [here](https://www.tensorflow.org/install/source#gpu).
> [TensorFlow Documentation](#https://www.tensorflow.org/install/pip#linux) advice NOT to use Conda to install TensorFlow since it may not have the latest stable version. TensorFlow is only officially released to PyPI.

#### Install TensorFlow with `conda`

> [TensorFlow Documentation](#https://www.tensorflow.org/install/pip#linux) advice NOT to use Conda to install TensorFlow since it may not have the latest stable version. TensorFlow is only officially released to PyPI. Use `pip` to install TensorFlow inside a Conda or virtual environment as instructed in the steps above.

1. Create a Conda environment and specify the TensorFlow package you need to install
```bash
conda create -p <path-to-env> <package-name> <cudatoolkit-version>
```
  Check the compatibility of each TensorFlow version with Python and CUDA from [here](https://www.tensorflow.org/install/source#gpu).

  For example:
  ```bash
  conda create -n tf-gpu tensorflow cudatoolkit=11.2 python=3.9
  ```

#### Check GPU usage in Python using TensorFlow
```python
import tensorflow as tf
```
Check if CUDA is available. This statement should return `True` if CUDA is available.
```python
tf.test.is_built_with_cuda()
```
To confirm that TensorFlow is using the GPU
```python
tf.config.list_physical_devices('GPU')
```

source: https://www.tensorflow.org/api_docs/python/tf/test/is_built_with_cuda




For more details on using Jubail, you can refer to the [Documentation](https://crc-docs.abudhabi.nyu.edu/hpc/system/index.html)

> TODO:

> check quota

> package an environment

> run?

> troubleshooting: logging on to the same node to check job/node status

> keras https://towardsdatascience.com/how-to-traine-tensorflow-models-79426dabd304
